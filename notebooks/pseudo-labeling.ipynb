{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Pseudo-Labeling Flow",
   "id": "a48ee7ad8fc90e2a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This notebook implements an automated pseudo-labeling pipeline designed to streamline the annotation process for object detection and instance segmentation tasks. The tool iteratively improves model performance by using an initial model trained on a small set of manually annotated data to generate labels on new images, which can then be refined and used to retrain progressively better models.\n",
    "\n",
    "**A \"flow\" represents a complete pseudo labeling run with specific configuration settings (model type, initial dataset size, correction strategy), while \"iterations\" are the individual training cycles within each flow where new data is added and the model is retrained.**\n",
    "\n",
    "The features within this notebook include:\n",
    "- **Automated Pipeline**: Complete workflow from data preparation to model training\n",
    "- **Database Logging**: Database tracking for all iterations\n",
    "- **CVAT Integration**: For viewing and adjusting annotations\n",
    "- **Flexible Configuration**: Supports different model architectures and training settings\n",
    "- **Status Monitoring**: Real-time pipeline status and progress tracking\n",
    "\n",
    "\n",
    "\n",
    "Automatic Export to CVAT only tested and functional for Instance Segmentation + Object Detection"
   ],
   "id": "4363b46de49c818c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports",
   "id": "59d550d53f7c2f2d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T06:55:40.702388Z",
     "start_time": "2025-07-10T06:55:40.699930Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import onedl.client\n",
    "\n",
    "from pseudo_labeling import PseudoLabelingPipeline"
   ],
   "id": "bbc3cfd3336091f4",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Global Initalizers\n",
    "Configure the pipeline with your project-specific settings:\n"
   ],
   "id": "e6cb8c06c2f22a57"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T06:55:55.854196Z",
     "start_time": "2025-07-10T06:55:54.426822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pipeline = PseudoLabelingPipeline(\n",
    "    project_name=\"daniel-osman---streamlining-annotation-bootstrapping/testing\",\n",
    "    main_dataset_name=\"full-data:0\", #input only\n",
    "    initial_annotated_dataset_name=\"initial-annotations:0\",\n",
    "    validation_dataset=\"val:0\",\n",
    "    sample_size_per_iter=150,\n",
    "    current_flow = 0,\n",
    "    min_confidence=0.1,\n",
    "    local_path='/Users/daniel/Documents/2025 BEP - VBTI Data/testing',\n",
    "    cvat_project_id=88,\n",
    "    db_path=\"pseudo_labeling_metadata.db\"\n",
    ")\n",
    "\n",
    "print(\"Pipeline initialized\")\n"
   ],
   "id": "b02f84cdd5a34fbd",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-07-10 08:55:55.287\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mpull\u001B[0m:\u001B[36m838\u001B[0m - \u001B[1mPulling dataset full-data:0 from remote='daniel-osman---streamlining-annotation-bootstrapping/testing' with pull_policy=missing.\u001B[0m\n",
      "\u001B[32m2025-07-10 08:55:55.293\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mpull\u001B[0m:\u001B[36m858\u001B[0m - \u001B[1mDataset full-data:0 already exists in local store. Skipping\u001B[0m\n",
      "\u001B[32m2025-07-10 08:55:55.326\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mpull\u001B[0m:\u001B[36m838\u001B[0m - \u001B[1mPulling dataset initial-annotations:0 from remote='daniel-osman---streamlining-annotation-bootstrapping/testing' with pull_policy=missing.\u001B[0m\n",
      "\u001B[32m2025-07-10 08:55:55.329\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mpull\u001B[0m:\u001B[36m858\u001B[0m - \u001B[1mDataset initial-annotations:0 already exists in local store. Skipping\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming incomplete iteration 1 (status: MERGE_COMPLETE)\n",
      "============================================================\n",
      "GLOBAL INITIALIZATIONS INITIALIZED\n",
      "============================================================\n",
      "Project: daniel-osman---streamlining-annotation-bootstrapping/testing\n",
      "Main dataset: full-data:0\n",
      "Initial annotated dataset: initial-annotations:0\n",
      "Sample size per iteration: 150\n",
      "Selected flow: f0\n",
      "Initial annotated dataset contains: 50 samples\n",
      "Flow f0 already exists in database - ready to resume\n",
      "Last completed iteration: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-07-10 08:55:55.843\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mresolve_latest_version\u001B[0m:\u001B[36m582\u001B[0m - \u001B[1mResolved latest version of dataset train-f0 to 5 with remote='daniel-osman---streamlining-annotation-bootstrapping/testing'.\u001B[0m\n",
      "\u001B[32m2025-07-10 08:55:55.844\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mload\u001B[0m:\u001B[36m375\u001B[0m - \u001B[1mResolved latest version of dataset train-f0 to 5.\u001B[0m\n",
      "\u001B[32m2025-07-10 08:55:55.845\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mpull\u001B[0m:\u001B[36m838\u001B[0m - \u001B[1mPulling dataset train-f0:5 from remote='daniel-osman---streamlining-annotation-bootstrapping/testing' with pull_policy=missing.\u001B[0m\n",
      "\u001B[32m2025-07-10 08:55:55.849\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mpull\u001B[0m:\u001B[36m858\u001B[0m - \u001B[1mDataset train-f0:5 already exists in local store. Skipping\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset train-f0 exists with 50 samples\n",
      "Ready for iteration 1\n",
      "\n",
      "üîÑ Attempting auto-recovery...\n",
      "üîÑ Recovering state for f0 iteration 1...\n",
      "‚úì Recovered predicted_dataset_name: pseudo-f0-0--cpu--266a4:0\n",
      "‚úì Recovered manual_corrections_mode: False\n",
      "‚úì Recovery complete - ready to resume\n",
      "============================================================\n",
      "Pipeline initialized\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training Config\n",
    "Set up your model training parameters interactively:\n"
   ],
   "id": "34c21f427b8edc65"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T07:45:21.258839Z",
     "start_time": "2025-07-10T07:45:21.255964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Option 1: Interactive widget setup (uncomment to use)\n",
    "# train_cfg = pipeline.setup_training_config()\n",
    "\n",
    "\n",
    "# Option 2: Direct dictionary configuration (recommended for specific config)\n",
    "pipeline.train_cfg = {\n",
    "    'model_type': 'FasterRCNNConfig',\n",
    "    'task_type': 'object_detection',\n",
    "    'backbone': 'RESNET_50',\n",
    "    'epochs': 1,\n",
    "    'batch_size': 3,\n",
    "}\n",
    "\n",
    "print(\"Training configuration set:\")\n",
    "print(pipeline.train_cfg)\n"
   ],
   "id": "83c2514d8a26f4fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration set:\n",
      "{'model_type': 'FasterRCNNConfig', 'task_type': 'object_detection', 'backbone': 'RESNET_50', 'epochs': 1, 'batch_size': 3}\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "45938baed4f024d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# (1) Initial Flow, Training, and Evaluation Setup\n",
    "\n",
    "This step initiates the current flow and establishes the baseline model using your initial annotated dataset.\n",
    "1. Loads the initial annotated dataset.\n",
    "2. Created training set for the current flow.\n",
    "3. Trains the first baseline model (iteration 0) for the specified flow.\n",
    "4. Evaluates Model\n",
    "5. Logs Metadata (Only variables generated throughout the process of the pipeline are 'predicted_dataset_name', 'model_uid', 'evaluation_uid', 'evaluation_info')\n",
    "\n",
    "‚ö†Ô∏è ATTENTION: Skip this section if your current flow already exists and if you already have a baseline model"
   ],
   "id": "97d036474ce1b3e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pipeline.get_pipeline_status()",
   "id": "d4389e61995bb67b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.1 Train Initial Model and Evaluate on Validation Set\n",
    "Train the baseline model on your initial annotated dataset.\n",
    "Evaluate the initial model performance on the validation dataset.\n",
    "\n",
    "\n"
   ],
   "id": "711f8bb04544ab75"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If you want to use an existing model, then run: <span style=\"color:#d73a49; font-family:monospace;\">pipeline.log_iteration_0_external_model(\"smoky-shepherd-0\")</span>, then skip to section (2):\n",
   "id": "83b9a47acbc33bde"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pipeline.train_model()\n",
    "pipeline.evaluate_model()"
   ],
   "id": "2fac8063b9a4b09a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.2 Log\n",
    "Save all metadata for the initial training iteration to the database.\n"
   ],
   "id": "846d17d234faaadf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pipeline.log_iteration_0()\n",
    "print(\"Initial model training and evaluation complete, Current status:\")"
   ],
   "id": "89bef19e5c8088c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# (2) Pseudo-Labeling Iteration Workflow\n",
    "\n",
    "This step executes a complete pseudo-labeling iteration cycle using the model from the previous iteration to generate labels on new data.\n",
    "1. Sets up the next iteration with correction strategy (manual or automated).\n",
    "2. Samples new unlabeled data from the full dataset.\n",
    "3. Runs inference using the previous iteration's model to generate pseudo-labels.\n",
    "4. Handles corrections based on strategy: exports to CVAT for manual corrections OR merges pseudo-labels directly.\n",
    "5. Trains updated model on expanded dataset (original + new data).\n",
    "6. Evaluates the updated model performance.\n",
    "7. Logs iteration metadata to track progress and results.\n",
    "\n",
    "**‚ö†Ô∏è ATTENTION: Set 'manual_corrections=True' for CVAT workflow with human review, or 'manual_corrections=False' for fully automated pseudo-labeling**\n"
   ],
   "id": "a448c0a08eb5e7f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T06:56:22.424848Z",
     "start_time": "2025-07-10T06:56:22.421812Z"
    }
   },
   "cell_type": "code",
   "source": "pipeline.get_pipeline_status()",
   "id": "f288d681ff9da4ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PIPELINE STATUS REPORT\n",
      "==================================================\n",
      "Flow ID: f0\n",
      "Current Iteration: 1\n",
      "Current Status: MERGE_COMPLETE\n",
      "Training Dataset: train-f0\n",
      "Current Model UID: None\n",
      "Training Configuration: None\n",
      "Database Path: pseudo_labeling_metadata.db\n",
      "Sample Size Per Iteration: 150\n",
      "Minimum Confidence Threshold: 0.1\n",
      "\n",
      "RECENT ITERATIONS:\n",
      "  Iteration 1: MERGE_COMPLETE\n",
      "  Iteration 0: COMPLETED (completed: 2025-07-09 12:08:24)\n",
      "==================================================\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Local Initializer\n",
    "Configure the next iteration parameters"
   ],
   "id": "813231686bb6f992"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T06:56:38.161317Z",
     "start_time": "2025-07-10T06:56:32.442378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set manual_corrections=True for CVAT human review\n",
    "# Set manual_corrections=False for fully automated pseudo-labeling\n",
    "manual_corrections = False\n",
    "pipeline.setup_next_iteration(manual_corrections)\n"
   ],
   "id": "9a5fa58ccb1fd960",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration 1 status: MERGE_COMPLETE\n",
      "Continuing with current iteration...\n",
      "----------------------------------------\n",
      "ITERATION INITIALIZED - PERSISTENT ARCHITECTURE\n",
      "========================================\n",
      "Flow ID: f0\n",
      "Current iteration: 1\n",
      "Manual corrections: False\n",
      "Sample size this iteration: 150\n",
      "GT added this iteration: 0\n",
      "Pseudo added this iteration: 150\n",
      "Total GT images after this step: 50\n",
      "Total pseudo-labeled images after this step: 150\n",
      "Total expected training set size: 200\n",
      "Train dataset name: train-f0\n",
      "Persistent pseudo dataset: pseudo-f0\n",
      "Manual corrections dataset: manual-corrections-f0\n",
      "Pseudo input dataset: pseudo-f0\n",
      "Initial annotations: initial-annotations:0\n",
      "Inference model UID: teal-ellipsis-0\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.1 Sample New Data",
   "id": "5e11a14b96cfb09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pipeline.sample_unseen_inputs()\n",
   "id": "7b5a6c0842156175",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.2 Generate Predictions/Pseudo-Labels\n",
   "id": "3ce8b829263e762f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# pipeline.set_inference_model_uid('')\n",
    "# pipeline.set_predicted_dataset('broccoli-semantic-segmentation-part4-may23-train-0--cpu--75080')\n",
    "\n",
    "pipeline.run_inference()"
   ],
   "id": "7e63fae3319becfc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.3 CVAT Export\n",
    "Run even if manual correction is false."
   ],
   "id": "dc43711977711a65"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if pipeline.manual_corrections_global:\n",
    "    print(\"Manual corrections enabled - proceeding to CVAT export\")\n",
    "    pipeline.manually_correct_cvat()\n",
    "    print(\"After completing corrections in CVAT, manually update the predicted dataset and run the merge cell below\")\n",
    "else:\n",
    "    print(\"No Manual Correction, Proceed to merging the datasets\")\n"
   ],
   "id": "f8bc6bab0ca05bfa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.4. Merge Data\n",
    "This cell merges the dataset with current training set. If **manual_correction = True**, then corrected annotations will be exported and merged.\n"
   ],
   "id": "209f3870a60460dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T06:56:57.802478Z",
     "start_time": "2025-07-10T06:56:55.186295Z"
    }
   },
   "cell_type": "code",
   "source": "pipeline.merge_pseudo_labels()",
   "id": "3a2c98c6bda8e396",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-07-10 08:56:55.188\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mpull\u001B[0m:\u001B[36m838\u001B[0m - \u001B[1mPulling dataset initial-annotations:0 from remote='daniel-osman---streamlining-annotation-bootstrapping/testing' with pull_policy=missing.\u001B[0m\n",
      "\u001B[32m2025-07-10 08:56:55.191\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mpull\u001B[0m:\u001B[36m858\u001B[0m - \u001B[1mDataset initial-annotations:0 already exists in local store. Skipping\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting simplified merge process...\n",
      "\n",
      "=== AUTO PSEUDO-LABELING MODE ===\n",
      "‚úì Pseudo dataset already updated after inference\n",
      "\n",
      "=== REBUILDING TRAINING DATASET (SIMPLIFIED) ===\n",
      "‚úì Started with initial dataset: 50 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-07-10 08:56:55.647\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mresolve_latest_version\u001B[0m:\u001B[36m576\u001B[0m - \u001B[1mThere is no remote version. Resolved latest version of dataset manual-corrections-f0 to 0 local='daniel-osman---streamlining-annotation-bootstrapping/testing'\u001B[0m\n",
      "\u001B[32m2025-07-10 08:56:55.648\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mload\u001B[0m:\u001B[36m375\u001B[0m - \u001B[1mResolved latest version of dataset manual-corrections-f0 to 0.\u001B[0m\n",
      "\u001B[32m2025-07-10 08:56:55.648\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mpull\u001B[0m:\u001B[36m838\u001B[0m - \u001B[1mPulling dataset manual-corrections-f0:0 from remote='daniel-osman---streamlining-annotation-bootstrapping/testing' with pull_policy=missing.\u001B[0m\n",
      "\u001B[32m2025-07-10 08:56:55.649\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mpull\u001B[0m:\u001B[36m849\u001B[0m - \u001B[1mPulling dataset manual-corrections-f0:0 from remote. pull_policy=<PullPolicy.missing: 'missing'> and dataset not found in local store.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì No manual corrections found for this flow: This resource cannot be found. GET https://api.onedl.ai/v2/storage/contexts/daniel-osman---streamlining-annotation-bootstrapping/testing/-/datasets/manual-corrections-f0:0/info\n",
      "404 Not Found - {'detail': 'Dataset manual-corrections-f0:0 was not found in project daniel-osman---streamlining-annotation-bootstrapping/testing.'}\n",
      "Received Body b'{\"detail\":\"Dataset manual-corrections-f0:0 was not found in project daniel-osman---streamlining-annotation-bootstrapping/testing.\"}'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-07-10 08:56:56.200\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mresolve_latest_version\u001B[0m:\u001B[36m582\u001B[0m - \u001B[1mResolved latest version of dataset pseudo-f0 to 0 with remote='daniel-osman---streamlining-annotation-bootstrapping/testing'.\u001B[0m\n",
      "\u001B[32m2025-07-10 08:56:56.200\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mload\u001B[0m:\u001B[36m375\u001B[0m - \u001B[1mResolved latest version of dataset pseudo-f0 to 0.\u001B[0m\n",
      "\u001B[32m2025-07-10 08:56:56.201\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mpull\u001B[0m:\u001B[36m838\u001B[0m - \u001B[1mPulling dataset pseudo-f0:0 from remote='daniel-osman---streamlining-annotation-bootstrapping/testing' with pull_policy=missing.\u001B[0m\n",
      "\u001B[32m2025-07-10 08:56:56.203\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mpull\u001B[0m:\u001B[36m858\u001B[0m - \u001B[1mDataset pseudo-f0:0 already exists in local store. Skipping\u001B[0m\n",
      "\u001B[32m2025-07-10 08:56:56.210\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mpull\u001B[0m:\u001B[36m838\u001B[0m - \u001B[1mPulling dataset initial-annotations:0 from remote='daniel-osman---streamlining-annotation-bootstrapping/testing' with pull_policy=missing.\u001B[0m\n",
      "\u001B[32m2025-07-10 08:56:56.211\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mpull\u001B[0m:\u001B[36m858\u001B[0m - \u001B[1mDataset initial-annotations:0 already exists in local store. Skipping\u001B[0m\n",
      "\u001B[32m2025-07-10 08:56:56.241\u001B[0m | \u001B[33m\u001B[1mWARNING \u001B[0m | \u001B[36monedl.datasets.columns.base_column\u001B[0m:\u001B[36mgenerate_label_map\u001B[0m:\u001B[36m461\u001B[0m - \u001B[33m\u001B[1mIterating over all elements in the column to generate label map. This can be slow for large columns. Consider `dataset['my_col'].freeze_label_map({0: 'dog'})` or `dataset['my_col'].freeze_labels(['dog'])` to skip label map generation.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Added pseudo dataset: 150 images, total: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating label map from unique labels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:00<00:00, 15570.21it/s]\n",
      "\u001B[32m2025-07-10 08:56:56.255\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl.datasets.columns.base_column\u001B[0m:\u001B[36m_generate_label_map_from_unique_labels\u001B[0m:\u001B[36m457\u001B[0m - \u001B[1mGenerated label map: {0: 'AboveGround', 1: 'Defect', 2: 'Overgrown', 3: 'Stone', 4: 'Tip'}\u001B[0m\n",
      "\u001B[32m2025-07-10 08:56:56.266\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36msave\u001B[0m:\u001B[36m179\u001B[0m - \u001B[1mSaved dataset train-f0:6 to local store.\u001B[0m\n",
      "\u001B[32m2025-07-10 08:56:56.267\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mresolve_latest_version\u001B[0m:\u001B[36m567\u001B[0m - \u001B[1mResolved latest version of dataset train-f0 to 6 local='daniel-osman---streamlining-annotation-bootstrapping/testing'\u001B[0m\n",
      "\u001B[32m2025-07-10 08:56:56.316\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m419\u001B[0m - \u001B[1mPushing 250 blobs to remote.\u001B[0m\n",
      "Files Uploaded:   0%|          | 0/251 [00:00<?, ?file/s]\n",
      "Files Confirmed:   0%|          | 0/251 [00:00<?, ?file/s]\u001B[A\n",
      "\n",
      "Getting upload links:   0%|          | 0/251 [00:00<?, ?file/s]\u001B[A\u001B[A\n",
      "\n",
      "Getting upload links: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [00:00<00:00, 891.54file/s]\u001B[A\u001B[A\n",
      "\n",
      "Files Uploaded: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 250/251 [00:00<00:00, 875.84file/s]      \u001B[A\u001B[A\n",
      "Files Confirmed: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 250/251 [00:00<00:00, 875.38file/s]\u001B[A\n",
      "\n",
      "Confirming files:   0%|          | 0/1 [00:00<?, ?file/s]\u001B[A\u001B[A\n",
      "\n",
      "Confirming files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.45file/s]\u001B[A\u001B[A\n",
      "\n",
      "                                                                    \u001B[A\r\n",
      "                                                                     \u001B[A\u001B[32m2025-07-10 08:56:57.398\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m430\u001B[0m - \u001B[1mPushing dataset train-f0:6 to remote.\u001B[0m\n",
      "\u001B[32m2025-07-10 08:56:57.796\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m449\u001B[0m - \u001B[1mPushed dataset remote as train-f0:6 -> train-f0:6.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì TRAINING DATASET REBUILT:\n",
      "  - Initial GT: 50 images\n",
      "  - Pseudo labels: 150 images\n",
      "  - Total: 200 images\n",
      "  - Saved as: train-f0\n",
      "  - Label map: {0: 'AboveGround', 1: 'Defect', 2: 'Overgrown', 3: 'Stone', 4: 'Tip'}\n",
      "==================================================\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.5 Train Updated Model\n",
    "Train a new model on the expanded training set"
   ],
   "id": "b501662afac7aa99"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T09:54:50.960002Z",
     "start_time": "2025-07-10T09:54:50.957326Z"
    }
   },
   "cell_type": "code",
   "source": "pipeline.train_model()",
   "id": "7e822708055ad554",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Training already completed. Recovered model UID: insulated-aggregate-0\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.6 Evaluate Performance",
   "id": "7a4feb9510ede0a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pipeline.evaluate_model()",
   "id": "daa83206d911d917",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.7 Status",
   "id": "1357a4f3c2cd78ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pipeline.get_pipeline_status()",
   "id": "843955a711b93b5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Additional Runs\n",
    "To run additional iterations, repeat Section 2 after logging. For creating a new flow, go back to Section 1, update the current_flow and go again.\n"
   ],
   "id": "bcb41c36fe5ccd64"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
