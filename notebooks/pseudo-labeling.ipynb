{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Pseudo-Labeling Flow",
   "id": "a48ee7ad8fc90e2a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This notebook implements an automated pseudo-labeling pipeline designed to streamline the annotation process for object detection and instance segmentation tasks. The tool iteratively improves model performance by using an initial model trained on a small set of manually annotated data to generate labels on new images, which can then be refined and used to retrain progressively better models.\n",
    "\n",
    "**A \"flow\" represents a complete pseudo labeling run with specific configuration settings (model type, initial dataset size, correction strategy), while \"iterations\" are the individual training cycles within each flow where new data is added and the model is retrained.**\n",
    "\n",
    "The features within this notebook include:\n",
    "- **Automated Pipeline**: Complete workflow from data preparation to model training\n",
    "- **Database Logging**: Database tracking for all iterations\n",
    "- **CVAT Integration**: For viewing and adjusting annotations\n",
    "- **Flexible Configuration**: Supports different model architectures and training settings\n",
    "- **Status Monitoring**: Real-time pipeline status and progress tracking\n",
    "\n",
    "\n",
    "\n",
    "Automatic Export to CVAT only tested and functional for Instance Segmentation + Object Detection"
   ],
   "id": "4363b46de49c818c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports",
   "id": "59d550d53f7c2f2d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T14:02:16.025058Z",
     "start_time": "2025-07-11T14:02:14.853420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import onedl.client\n",
    "\n",
    "from pseudo_labeling import PseudoLabelingPipeline"
   ],
   "id": "bbc3cfd3336091f4",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Global Initalizers\n",
    "Configure the pipeline with your project-specific settings:\n"
   ],
   "id": "e6cb8c06c2f22a57"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T14:02:20.310796Z",
     "start_time": "2025-07-11T14:02:16.921325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pipeline = PseudoLabelingPipeline(\n",
    "    project_name=\"daniel-osman---streamlining-annotation-bootstrapping/pipeline-test\",\n",
    "    main_dataset_name=\"full-dataset:0\", #input only\n",
    "    initial_annotated_dataset_name=\"initial-annotations:0\",\n",
    "    validation_dataset=\"val:0\",\n",
    "    sample_size_per_iter=150,\n",
    "    current_flow = 0,\n",
    "    min_confidence=0.5,\n",
    "    local_path='',\n",
    "    cvat_project_id=88,#Ignore for now\n",
    "    db_path=\"pseudo_labeling_metadata_ptest.db\"\n",
    ")\n",
    "\n",
    "print(\"Pipeline initialized\")\n"
   ],
   "id": "b02f84cdd5a34fbd",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-07-11 16:02:17.486\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mpull\u001B[0m:\u001B[36m848\u001B[0m - \u001B[1mPulling dataset full-dataset:0 from remote='daniel-osman---streamlining-annotation-bootstrapping/pipeline-test' with pull_policy=missing.\u001B[0m\n",
      "\u001B[32m2025-07-11 16:02:17.492\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mpull\u001B[0m:\u001B[36m868\u001B[0m - \u001B[1mDataset full-dataset:0 already exists in local store. Skipping\u001B[0m\n",
      "\u001B[32m2025-07-11 16:02:19.863\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mpull\u001B[0m:\u001B[36m848\u001B[0m - \u001B[1mPulling dataset initial-annotations:0 from remote='daniel-osman---streamlining-annotation-bootstrapping/pipeline-test' with pull_policy=missing.\u001B[0m\n",
      "\u001B[32m2025-07-11 16:02:19.865\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mpull\u001B[0m:\u001B[36m868\u001B[0m - \u001B[1mDataset initial-annotations:0 already exists in local store. Skipping\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GLOBAL INITIALIZATIONS INITIALIZED\n",
      "============================================================\n",
      "Project: daniel-osman---streamlining-annotation-bootstrapping/pipeline-test\n",
      "Main dataset: full-dataset:0\n",
      "Initial annotated dataset: initial-annotations:0\n",
      "Sample size per iteration: 150\n",
      "Selected flow: f0\n",
      "Initial annotated dataset contains: 50 samples\n",
      "Flow f0 already exists in database - ready to resume\n",
      "Last completed iteration: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-07-11 16:02:20.282\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mresolve_latest_version\u001B[0m:\u001B[36m592\u001B[0m - \u001B[1mResolved latest version of dataset train-f0 to 2 with remote='daniel-osman---streamlining-annotation-bootstrapping/pipeline-test'.\u001B[0m\n",
      "\u001B[32m2025-07-11 16:02:20.283\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mload\u001B[0m:\u001B[36m385\u001B[0m - \u001B[1mResolved latest version of dataset train-f0 to 2.\u001B[0m\n",
      "\u001B[32m2025-07-11 16:02:20.283\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mpull\u001B[0m:\u001B[36m848\u001B[0m - \u001B[1mPulling dataset train-f0:2 from remote='daniel-osman---streamlining-annotation-bootstrapping/pipeline-test' with pull_policy=missing.\u001B[0m\n",
      "\u001B[32m2025-07-11 16:02:20.288\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mpull\u001B[0m:\u001B[36m868\u001B[0m - \u001B[1mDataset train-f0:2 already exists in local store. Skipping\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset train-f0 exists with 150 samples\n",
      "Ready for iteration 2\n",
      "\n",
      "Attempting auto-recovery...\n",
      "Recovering state for f0 iteration 2...\n",
      "No database record found - this appears to be a new iteration\n",
      "============================================================\n",
      "Pipeline initialized\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training Config\n",
    "Set up your model training parameters interactively:\n"
   ],
   "id": "34c21f427b8edc65"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T14:02:23.886462Z",
     "start_time": "2025-07-11T14:02:23.883598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Option 1: Interactive widget setup (uncomment to use)\n",
    "# train_cfg = pipeline.setup_training_config()\n",
    "\n",
    "\n",
    "# Option 2: Direct dictionary configuration (recommended for specific config)\n",
    "pipeline.train_cfg = {\n",
    "    'model_type': 'FasterRCNNConfig',\n",
    "    'task_type': 'object_detection',\n",
    "    'backbone': 'RESNET_50',\n",
    "    'epochs': 6,\n",
    "    'batch_size': 6,\n",
    "}\n",
    "\n",
    "print(\"Training configuration set:\")\n",
    "print(pipeline.train_cfg)\n"
   ],
   "id": "83c2514d8a26f4fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration set:\n",
      "{'model_type': 'FasterRCNNConfig', 'task_type': 'object_detection', 'backbone': 'RESNET_50', 'epochs': 6, 'batch_size': 6}\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "45938baed4f024d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# (1) Initial Flow, Training, and Evaluation Setup\n",
    "\n",
    "This step initiates the current flow and establishes the baseline model using your initial annotated dataset.\n",
    "1. Loads the initial annotated dataset.\n",
    "2. Created training set for the current flow.\n",
    "3. Trains the first baseline model (iteration 0) for the specified flow.\n",
    "4. Evaluates Model\n",
    "5. Logs Metadata (Only variables generated throughout the process of the pipeline are 'predicted_dataset_name', 'model_uid', 'evaluation_uid', 'evaluation_info')\n",
    "\n",
    "⚠️ ATTENTION: Skip this section if your current flow already exists and if you already have a baseline model"
   ],
   "id": "97d036474ce1b3e8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T14:02:26.426732Z",
     "start_time": "2025-07-11T14:02:26.423908Z"
    }
   },
   "cell_type": "code",
   "source": "pipeline.get_pipeline_status()",
   "id": "d4389e61995bb67b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PIPELINE STATUS REPORT\n",
      "==================================================\n",
      "Flow ID: f0\n",
      "Current Iteration: 2\n",
      "Training Dataset: train-f0\n",
      "Current Model UID: None\n",
      "Training Configuration: {'model_type': 'FasterRCNNConfig', 'task_type': 'object_detection', 'backbone': 'RESNET_50', 'epochs': 6, 'batch_size': 6}\n",
      "Database Path: pseudo_labeling_metadata_ptest.db\n",
      "Sample Size Per Iteration: 150\n",
      "Minimum Confidence Threshold: 0.5\n",
      "\n",
      "RECENT ITERATIONS:\n",
      "  Iteration 1: COMPLETED (completed: 2025-07-11 15:14:48)\n",
      "  Iteration 0: COMPLETED (completed: 2025-07-11 13:21:04)\n",
      "==================================================\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.1 Train Initial Model and Evaluate on Validation Set\n",
    "Train the baseline model on your initial annotated dataset.\n",
    "Evaluate the initial model performance on the validation dataset.\n",
    "\n",
    "\n"
   ],
   "id": "711f8bb04544ab75"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If you want to use an existing model, then run: <span style=\"color:#d73a49; font-family:monospace;\">pipeline.log_iteration_0_external_model(\"smoky-shepherd-0\")</span>, then skip to section (2):\n",
   "id": "83b9a47acbc33bde"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T11:20:58.867264Z",
     "start_time": "2025-07-11T11:03:57.755680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pipeline.train_model()\n",
    "pipeline.evaluate_model()"
   ],
   "id": "2fac8063b9a4b09a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training...\n",
      "Training FasterRCNNConfig on dataset: initial-annotations:0\n",
      "Configuration: 30 epochs, batch size 6\n",
      "Backbone: RESNET_50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-07-11 13:04:00.571\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl.client.operations.clients._common\u001B[0m:\u001B[36mcreate_event_stream\u001B[0m:\u001B[36m79\u001B[0m - \u001B[1mSubscribing to job events...\u001B[0m\n",
      "\u001B[32m2025-07-11 13:04:00.572\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl.client.operations.clients._common\u001B[0m:\u001B[36mcreate_event_stream\u001B[0m:\u001B[36m80\u001B[0m - \u001B[1mJob rounded-type-0 in WAITING state\u001B[0m\n",
      "\u001B[32m2025-07-11 13:04:01.594\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl.client.operations.clients._common\u001B[0m:\u001B[36mcreate_event_stream\u001B[0m:\u001B[36m84\u001B[0m - \u001B[1mJob rounded-type-0 in RUNNING state\u001B[0m\n",
      "\u001B[32m2025-07-11 13:04:01.596\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl.client.operations.clients._common\u001B[0m:\u001B[36mcreate_event_stream\u001B[0m:\u001B[36m84\u001B[0m - \u001B[1mJob rounded-type-0 in RUNNING state\u001B[0m\n",
      "\u001B[32m2025-07-11 13:15:12.655\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl.client.operations.clients._common\u001B[0m:\u001B[36mcreate_event_stream\u001B[0m:\u001B[36m84\u001B[0m - \u001B[1mJob rounded-type-0 in DONE state\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job submitted\n",
      "Model UID: rounded-type-0\n",
      "Training job state: DONE\n",
      "Evaluating rounded-type-0 on val:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-07-11 13:15:16.340\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl.client.operations.clients._common\u001B[0m:\u001B[36mcreate_event_stream\u001B[0m:\u001B[36m79\u001B[0m - \u001B[1mSubscribing to job events...\u001B[0m\n",
      "\u001B[32m2025-07-11 13:15:16.342\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl.client.operations.clients._common\u001B[0m:\u001B[36mcreate_event_stream\u001B[0m:\u001B[36m80\u001B[0m - \u001B[1mJob rigid-spread-0 in WAITING state\u001B[0m\n",
      "\u001B[32m2025-07-11 13:15:17.158\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl.client.operations.clients._common\u001B[0m:\u001B[36mcreate_event_stream\u001B[0m:\u001B[36m84\u001B[0m - \u001B[1mJob rigid-spread-0 in RUNNING state\u001B[0m\n",
      "\u001B[32m2025-07-11 13:20:56.437\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl.client.operations.clients._common\u001B[0m:\u001B[36mcreate_event_stream\u001B[0m:\u001B[36m84\u001B[0m - \u001B[1mJob rigid-spread-0 in DONE state\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation job submitted\n",
      "Evaluation UID: rigid-spread-0\n",
      "Evaluation job state: DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete\n",
      "Report URL: https://21e007818fa1dd0840eac0d6d59ba986.eu.r2.cloudflarestorage.com/onedl-data/daniel-osman---streamlining-annotation-bootstrapping/pipeline-test/-/92699172c2ef5c4f9aeb58a06eab97f0.html?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=bb17714b86b2e84a836c55404335cef8%2F20250711%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20250711T112058Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=439b20968c66e259d9e9f47a04e83258c9a66d2d0c7f1993871493f7af17c5ae\n",
      "Metrics: {\"mAP50\": 0.05244764684098055, \"mAP75\": 0.0014695996497915195, \"mAP_all\": 0.019946033489825075, \"fn_count\": 72, \"fp_count\": 359, \"tp_count\": 80}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.2 Log\n",
    "Save all metadata for the initial training iteration to the database.\n"
   ],
   "id": "846d17d234faaadf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T11:21:04.864410Z",
     "start_time": "2025-07-11T11:21:04.861425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pipeline.log_iteration_0() # This is required now but will be removed\n",
    "print(\"Initial model training and evaluation complete, Current status:\")"
   ],
   "id": "89bef19e5c8088c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 logged for f0\n",
      "Initial model training and evaluation complete, Current status:\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# (2) Pseudo-Labeling Iteration Workflow\n",
    "\n",
    "This step executes a complete pseudo-labeling iteration cycle using the model from the previous iteration to generate labels on new data.\n",
    "1. Sets up the next iteration with correction strategy (manual or automated).\n",
    "2. Samples new unlabeled data from the full dataset.\n",
    "3. Runs inference using the previous iteration's model to generate pseudo-labels.\n",
    "4. Handles corrections based on strategy: exports to CVAT for manual corrections OR merges pseudo-labels directly.\n",
    "5. Trains updated model on expanded dataset (original + new data).\n",
    "6. Evaluates the updated model performance.\n",
    "7. Logs iteration metadata to track progress and results.\n",
    "\n",
    "**⚠️ ATTENTION: Set 'manual_corrections=True' for CVAT workflow with human review, or 'manual_corrections=False' for fully automated pseudo-labeling**\n"
   ],
   "id": "a448c0a08eb5e7f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T14:01:12.366026Z",
     "start_time": "2025-07-11T14:01:12.363233Z"
    }
   },
   "cell_type": "code",
   "source": "pipeline.get_pipeline_status()",
   "id": "f288d681ff9da4ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PIPELINE STATUS REPORT\n",
      "==================================================\n",
      "Flow ID: f0\n",
      "Current Iteration: 2\n",
      "Training Dataset: train-f0\n",
      "Current Model UID: None\n",
      "Training Configuration: {'model_type': 'FasterRCNNConfig', 'task_type': 'object_detection', 'backbone': 'RESNET_50', 'epochs': 6, 'batch_size': 6}\n",
      "Database Path: pseudo_labeling_metadata_ptest.db\n",
      "Sample Size Per Iteration: 150\n",
      "Minimum Confidence Threshold: 0.5\n",
      "\n",
      "RECENT ITERATIONS:\n",
      "  Iteration 1: COMPLETED (completed: 2025-07-11 15:14:48)\n",
      "  Iteration 0: COMPLETED (completed: 2025-07-11 13:21:04)\n",
      "==================================================\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Local Initializer\n",
    "Configure the next iteration parameters\n",
    "\n",
    "Set `manual_corrections = True` for CVAT manual review.\n",
    "\n",
    "Set `manual_corrections = False` for fully automated pseudo-labeling.\n"
   ],
   "id": "813231686bb6f992"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T14:02:33.830973Z",
     "start_time": "2025-07-11T14:02:33.827680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "manual_corrections = False\n",
    "pipeline.setup_next_iteration(manual_corrections)\n"
   ],
   "id": "9a5fa58ccb1fd960",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No status found for iteration 2. Proceeding...\n",
      "----------------------------------------\n",
      "ITERATION INITIALIZED - PERSISTENT ARCHITECTURE\n",
      "========================================\n",
      "Flow ID: f0\n",
      "Current iteration: 2\n",
      "Manual corrections: False\n",
      "Sample size this iteration: 150\n",
      "GT added this iteration: 0\n",
      "Pseudo added this iteration: 150\n",
      "Total GT images after this step: 50\n",
      "Total pseudo-labeled images after this step: 300\n",
      "Total expected training set size: 350\n",
      "Train dataset name: train-f0\n",
      "Persistent pseudo dataset: pseudo-f0\n",
      "Manual corrections dataset: manual-corrections-f0\n",
      "Pseudo input dataset: pseudo-f0\n",
      "Initial annotations: initial-annotations:0\n",
      "Inference model UID: chill-muffler-0\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.1 Sample New Data",
   "id": "5e11a14b96cfb09"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T14:03:01.146496Z",
     "start_time": "2025-07-11T14:02:36.036823Z"
    }
   },
   "cell_type": "code",
   "source": "pipeline.sample_unseen_inputs()\n",
   "id": "7b5a6c0842156175",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SAMPLING FOR RE-INFERENCE (ITERATION 2) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-07-11 16:02:36.459\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mresolve_latest_version\u001B[0m:\u001B[36m592\u001B[0m - \u001B[1mResolved latest version of dataset train-f0 to 2 with remote='daniel-osman---streamlining-annotation-bootstrapping/pipeline-test'.\u001B[0m\n",
      "\u001B[32m2025-07-11 16:02:36.459\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mload\u001B[0m:\u001B[36m385\u001B[0m - \u001B[1mResolved latest version of dataset train-f0 to 2.\u001B[0m\n",
      "\u001B[32m2025-07-11 16:02:36.459\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mpull\u001B[0m:\u001B[36m848\u001B[0m - \u001B[1mPulling dataset train-f0:2 from remote='daniel-osman---streamlining-annotation-bootstrapping/pipeline-test' with pull_policy=missing.\u001B[0m\n",
      "\u001B[32m2025-07-11 16:02:36.463\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mpull\u001B[0m:\u001B[36m868\u001B[0m - \u001B[1mDataset train-f0:2 already exists in local store. Skipping\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Current training dataset has 150 images\n",
      "Total images in full dataset: 12630\n",
      "Images already in training: 150\n",
      "Images NOT in training (available): 12480\n",
      "✓ Sampled 150 new images from unused set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-07-11 16:02:36.740\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mresolve_latest_version\u001B[0m:\u001B[36m592\u001B[0m - \u001B[1mResolved latest version of dataset pseudo-f0 to 4 with remote='daniel-osman---streamlining-annotation-bootstrapping/pipeline-test'.\u001B[0m\n",
      "\u001B[32m2025-07-11 16:02:36.740\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mload\u001B[0m:\u001B[36m385\u001B[0m - \u001B[1mResolved latest version of dataset pseudo-f0 to 4.\u001B[0m\n",
      "\u001B[32m2025-07-11 16:02:36.741\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mpull\u001B[0m:\u001B[36m848\u001B[0m - \u001B[1mPulling dataset pseudo-f0:4 from remote='daniel-osman---streamlining-annotation-bootstrapping/pipeline-test' with pull_policy=missing.\u001B[0m\n",
      "\u001B[32m2025-07-11 16:02:36.743\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mpull\u001B[0m:\u001B[36m868\u001B[0m - \u001B[1mDataset pseudo-f0:4 already exists in local store. Skipping\u001B[0m\n",
      "\u001B[32m2025-07-11 16:02:36.764\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36msave\u001B[0m:\u001B[36m191\u001B[0m - \u001B[1mSaved dataset pseudo-f0:5 to local store.\u001B[0m\n",
      "\u001B[32m2025-07-11 16:02:36.765\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mresolve_latest_version\u001B[0m:\u001B[36m577\u001B[0m - \u001B[1mResolved latest version of dataset pseudo-f0 to 5 local='daniel-osman---streamlining-annotation-bootstrapping/pipeline-test'\u001B[0m\n",
      "\u001B[32m2025-07-11 16:02:36.794\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m429\u001B[0m - \u001B[1mPushing 296 blobs to remote.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Found existing pseudo dataset: 150 images\n",
      "✓ Combined: 150 existing + 150 new = 300 total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files Uploaded:   0%|          | 0/297 [00:00<?, ?file/s]\n",
      "Files Confirmed:   0%|          | 0/297 [00:00<?, ?file/s]\u001B[A\n",
      "\n",
      "Getting upload links:   0%|          | 0/297 [00:00<?, ?file/s]\u001B[A\u001B[A\n",
      "\n",
      "Getting upload links:  86%|████████▌ | 256/297 [00:00<00:00, 644.63file/s]\u001B[A\u001B[A\n",
      "\n",
      "Files Uploaded:  53%|█████▎    | 156/297 [00:00<00:00, 228.81file/s]      \u001B[A\u001B[A\n",
      "Files Uploaded:  73%|███████▎  | 218/297 [00:10<00:09,  8.66file/s] ]\u001B[A\n",
      "Files Uploaded: 100%|██████████| 297/297 [00:22<00:00, 10.45file/s]s]\u001B[A\n",
      "\n",
      "Confirming files:   0%|          | 0/141 [00:00<?, ?file/s]\u001B[A\u001B[A\n",
      "\n",
      "Confirming files: 100%|██████████| 141/141 [00:01<00:00, 137.48file/s]\u001B[A\u001B[A\n",
      "\n",
      "                                                                      \u001B[A\u001B[A\n",
      "                                                                   ] \u001B[A\r\n",
      "                                                                    \u001B[A\u001B[32m2025-07-11 16:03:00.656\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m440\u001B[0m - \u001B[1mPushing dataset pseudo-f0:5 to remote.\u001B[0m\n",
      "\u001B[32m2025-07-11 16:03:01.140\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m459\u001B[0m - \u001B[1mPushed dataset remote as pseudo-f0:5 -> pseudo-f0:5.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved inputs-only dataset: pseudo-f0\n",
      "✓ Ready for re-inference on 300 images with evolved model\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.2 Generate Predictions/Pseudo-Labels\n",
   "id": "3ce8b829263e762f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T14:12:11.433565Z",
     "start_time": "2025-07-11T14:03:10.810104Z"
    }
   },
   "cell_type": "code",
   "source": "pipeline.run_inference()",
   "id": "7e63fae3319becfc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference with model: chill-muffler-0\n",
      "Running inference on persistent pseudo dataset: pseudo-f0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-07-11 16:03:11.443\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mresolve_latest_version\u001B[0m:\u001B[36m592\u001B[0m - \u001B[1mResolved latest version of dataset pseudo-f0 to 5 with remote='daniel-osman---streamlining-annotation-bootstrapping/pipeline-test'.\u001B[0m\n",
      "\u001B[32m2025-07-11 16:03:12.015\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mresolve_latest_version\u001B[0m:\u001B[36m592\u001B[0m - \u001B[1mResolved latest version of dataset pseudo-f0 to 5 with remote='daniel-osman---streamlining-annotation-bootstrapping/pipeline-test'.\u001B[0m\n",
      "\u001B[32m2025-07-11 16:03:13.528\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl.client.operations.clients._common\u001B[0m:\u001B[36mcreate_event_stream\u001B[0m:\u001B[36m79\u001B[0m - \u001B[1mSubscribing to job events...\u001B[0m\n",
      "\u001B[32m2025-07-11 16:03:13.530\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl.client.operations.clients._common\u001B[0m:\u001B[36mcreate_event_stream\u001B[0m:\u001B[36m80\u001B[0m - \u001B[1mJob ebony-body-0 in WAITING state\u001B[0m\n",
      "\u001B[32m2025-07-11 16:03:14.346\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl.client.operations.clients._common\u001B[0m:\u001B[36mcreate_event_stream\u001B[0m:\u001B[36m84\u001B[0m - \u001B[1mJob ebony-body-0 in RUNNING state\u001B[0m\n",
      "\u001B[32m2025-07-11 16:12:09.352\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl.client.operations.clients._common\u001B[0m:\u001B[36mcreate_event_stream\u001B[0m:\u001B[36m84\u001B[0m - \u001B[1mJob ebony-body-0 in FAILED state\u001B[0m\n",
      "\u001B[32m2025-07-11 16:12:09.852\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mpull\u001B[0m:\u001B[36m848\u001B[0m - \u001B[1mPulling dataset pseudo-f0-5--cpu--91f39:0 from remote='daniel-osman---streamlining-annotation-bootstrapping/pipeline-test' with pull_policy=missing.\u001B[0m\n",
      "\u001B[32m2025-07-11 16:12:09.854\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mpull\u001B[0m:\u001B[36m859\u001B[0m - \u001B[1mPulling dataset pseudo-f0-5--cpu--91f39:0 from remote. pull_policy=<PullPolicy.missing: 'missing'> and dataset not found in local store.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference complete\n",
      "Predictions saved as: pseudo-f0-5--cpu--91f39:0\n",
      "\n",
      "=== REPLACING PERSISTENT PSEUDO DATASET WITH PREDICTIONS ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-07-11 16:12:10.263\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.blobs\u001B[0m:\u001B[36mget_path_many\u001B[0m:\u001B[36m489\u001B[0m - \u001B[1mPulling 1/1 blobs from remote. pull_policy=<PullPolicy.missing: 'missing'>\u001B[0m\n",
      "Downloading files: 100%|██████████| 1/1 [00:00<00:00,  4.51file/s]     \n",
      "\u001B[32m2025-07-11 16:12:11.262\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36msave\u001B[0m:\u001B[36m139\u001B[0m - \u001B[1mOverwriting dataset `pseudo-f0`.\u001B[0m\n",
      "\u001B[32m2025-07-11 16:12:11.264\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mdelete_local\u001B[0m:\u001B[36m1147\u001B[0m - \u001B[1mversion=None\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Filtered predictions by confidence >= 0.5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Attempting to delete a dataset with unversioned name pseudo-f0, which has multiple versions. Please specify a version to delete, or pass delete_all_versions=True to delete all versions.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mpipeline\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_inference\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/VBTI - Pseudo.Labeling/pseudo_labeling.py:1062\u001B[39m, in \u001B[36mPseudoLabelingPipeline.run_inference\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1059\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mPredictions saved as: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.predicted_dataset_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m   1061\u001B[39m \u001B[38;5;66;03m# NEW: Replace the persistent pseudo dataset with filtered predictions\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1062\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_replace_persistent_pseudo_dataset_with_predictions\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1064\u001B[39m \u001B[38;5;66;03m# Update database\u001B[39;00m\n\u001B[32m   1065\u001B[39m \u001B[38;5;28mself\u001B[39m.db.update_iteration_field(\n\u001B[32m   1066\u001B[39m     \u001B[38;5;28mself\u001B[39m.flow_id, \u001B[38;5;28mself\u001B[39m.current_iteration,\n\u001B[32m   1067\u001B[39m     pseudo_output_dataset_name=\u001B[38;5;28mself\u001B[39m.predicted_dataset_name,\n\u001B[32m   1068\u001B[39m     status=\u001B[33m'\u001B[39m\u001B[33mINFERENCE_COMPLETE\u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m   1069\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/VBTI - Pseudo.Labeling/pseudo_labeling.py:1104\u001B[39m, in \u001B[36mPseudoLabelingPipeline._replace_persistent_pseudo_dataset_with_predictions\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1101\u001B[39m persistent_pseudo_name = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mpseudo-f\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.current_flow\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m   1103\u001B[39m \u001B[38;5;66;03m# CORRECTED: Replace the entire dataset instead of adding to it\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1104\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mclient\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdatasets\u001B[49m\u001B[43m.\u001B[49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpersistent_pseudo_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpredicted_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexist\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43moverwrite\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m   1105\u001B[39m \u001B[38;5;28mself\u001B[39m.client.datasets.push(persistent_pseudo_name, push_policy=\u001B[33m\"\u001B[39m\u001B[33mversion\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m   1107\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m✓ Replaced persistent pseudo dataset: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpersistent_pseudo_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv/lib/python3.11/site-packages/onedl/_local_store/datasets.py:140\u001B[39m, in \u001B[36mDatasetStore.save\u001B[39m\u001B[34m(self, name, dataset, project_path, exist, skip_validation)\u001B[39m\n\u001B[32m    138\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m exist == EXIST.OVERWRITE \u001B[38;5;129;01mand\u001B[39;00m exist_unversioned:\n\u001B[32m    139\u001B[39m     logger.info(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mOverwriting dataset `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m`.\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m140\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdelete_local\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mproject_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43mproject_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdelete_blobs\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m    142\u001B[39m label_maps = dataset._generate_or_validate_label_maps(skip_validation=skip_validation)\n\u001B[32m    143\u001B[39m label_maps = {col: label_map._class_id_to_label_map \u001B[38;5;28;01mfor\u001B[39;00m col, label_map \u001B[38;5;129;01min\u001B[39;00m label_maps.items()}\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv/lib/python3.11/site-packages/onedl/_local_store/datasets.py:1153\u001B[39m, in \u001B[36mDatasetStore.delete_local\u001B[39m\u001B[34m(self, name, delete_blobs, delete_external_blobs, delete_all_versions, project_path)\u001B[39m\n\u001B[32m   1147\u001B[39m     logger.info(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mversion\u001B[38;5;132;01m=}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m   1148\u001B[39m     message = (\n\u001B[32m   1149\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mAttempting to delete a dataset with unversioned name \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1150\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mwhich has multiple versions. Please specify a version to delete, \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1151\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mor pass delete_all_versions=True to delete all versions.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1152\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m1153\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(message)\n\u001B[32m   1154\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m num_snapshots > \u001B[32m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m delete_all_versions:\n\u001B[32m   1155\u001B[39m     logger.debug(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mDeleting \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_snapshots\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m versions of the dataset \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mValueError\u001B[39m: Attempting to delete a dataset with unversioned name pseudo-f0, which has multiple versions. Please specify a version to delete, or pass delete_all_versions=True to delete all versions."
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.3 CVAT Export\n",
    "Run even if manual correction is false."
   ],
   "id": "dc43711977711a65"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T11:35:07.259204Z",
     "start_time": "2025-07-11T11:35:07.256636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if pipeline.manual_corrections_global:\n",
    "    print(\"Manual corrections enabled - proceeding to CVAT export\")\n",
    "    pipeline.manually_correct_cvat()\n",
    "    print(\"After completing corrections in CVAT, manually update the predicted dataset and run the merge cell below\")\n",
    "else:\n",
    "    print(\"No Manual Correction, Proceed to merging the datasets\")\n"
   ],
   "id": "f8bc6bab0ca05bfa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Manual Correction, Proceed to merging the datasets\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.4. Merge Data\n",
    "This cell merges the dataset with current training set. If **manual_correction = True**, then corrected annotations will be exported and merged.\n"
   ],
   "id": "209f3870a60460dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T11:35:20.312742Z",
     "start_time": "2025-07-11T11:35:15.266005Z"
    }
   },
   "cell_type": "code",
   "source": "pipeline.merge_pseudo_labels()",
   "id": "3a2c98c6bda8e396",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-07-11 13:35:15.267\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mpull\u001B[0m:\u001B[36m848\u001B[0m - \u001B[1mPulling dataset initial-annotations:0 from remote='daniel-osman---streamlining-annotation-bootstrapping/pipeline-test' with pull_policy=missing.\u001B[0m\n",
      "\u001B[32m2025-07-11 13:35:15.271\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mpull\u001B[0m:\u001B[36m868\u001B[0m - \u001B[1mDataset initial-annotations:0 already exists in local store. Skipping\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting simplified merge process...\n",
      "\n",
      "=== AUTO PSEUDO-LABELING MODE ===\n",
      "✓ Pseudo dataset already updated after inference\n",
      "\n",
      "=== REBUILDING TRAINING DATASET (SIMPLIFIED) ===\n",
      "✓ Started with initial dataset: 50 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-07-11 13:35:15.855\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mresolve_latest_version\u001B[0m:\u001B[36m586\u001B[0m - \u001B[1mThere is no remote version. Resolved latest version of dataset manual-corrections-f0 to 0 local='daniel-osman---streamlining-annotation-bootstrapping/pipeline-test'\u001B[0m\n",
      "\u001B[32m2025-07-11 13:35:15.855\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mload\u001B[0m:\u001B[36m385\u001B[0m - \u001B[1mResolved latest version of dataset manual-corrections-f0 to 0.\u001B[0m\n",
      "\u001B[32m2025-07-11 13:35:15.855\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mpull\u001B[0m:\u001B[36m848\u001B[0m - \u001B[1mPulling dataset manual-corrections-f0:0 from remote='daniel-osman---streamlining-annotation-bootstrapping/pipeline-test' with pull_policy=missing.\u001B[0m\n",
      "\u001B[32m2025-07-11 13:35:15.856\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mpull\u001B[0m:\u001B[36m859\u001B[0m - \u001B[1mPulling dataset manual-corrections-f0:0 from remote. pull_policy=<PullPolicy.missing: 'missing'> and dataset not found in local store.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ No manual corrections found for this flow: This resource cannot be found. GET https://api.onedl.ai/v2/storage/contexts/daniel-osman---streamlining-annotation-bootstrapping/pipeline-test/-/datasets/manual-corrections-f0:0/info\n",
      "404 Not Found - {'detail': 'Dataset manual-corrections-f0:0 was not found in project daniel-osman---streamlining-annotation-bootstrapping/pipeline-test.'}\n",
      "Received Body b'{\"detail\":\"Dataset manual-corrections-f0:0 was not found in project daniel-osman---streamlining-annotation-bootstrapping/pipeline-test.\"}'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-07-11 13:35:16.373\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mresolve_latest_version\u001B[0m:\u001B[36m592\u001B[0m - \u001B[1mResolved latest version of dataset pseudo-f0 to 1 with remote='daniel-osman---streamlining-annotation-bootstrapping/pipeline-test'.\u001B[0m\n",
      "\u001B[32m2025-07-11 13:35:16.373\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mload\u001B[0m:\u001B[36m385\u001B[0m - \u001B[1mResolved latest version of dataset pseudo-f0 to 1.\u001B[0m\n",
      "\u001B[32m2025-07-11 13:35:16.374\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mpull\u001B[0m:\u001B[36m848\u001B[0m - \u001B[1mPulling dataset pseudo-f0:1 from remote='daniel-osman---streamlining-annotation-bootstrapping/pipeline-test' with pull_policy=missing.\u001B[0m\n",
      "\u001B[32m2025-07-11 13:35:16.376\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mpull\u001B[0m:\u001B[36m868\u001B[0m - \u001B[1mDataset pseudo-f0:1 already exists in local store. Skipping\u001B[0m\n",
      "\u001B[32m2025-07-11 13:35:16.393\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mpull\u001B[0m:\u001B[36m848\u001B[0m - \u001B[1mPulling dataset initial-annotations:0 from remote='daniel-osman---streamlining-annotation-bootstrapping/pipeline-test' with pull_policy=missing.\u001B[0m\n",
      "\u001B[32m2025-07-11 13:35:16.395\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mpull\u001B[0m:\u001B[36m868\u001B[0m - \u001B[1mDataset initial-annotations:0 already exists in local store. Skipping\u001B[0m\n",
      "\u001B[32m2025-07-11 13:35:16.446\u001B[0m | \u001B[33m\u001B[1mWARNING \u001B[0m | \u001B[36monedl.datasets.columns.base_column\u001B[0m:\u001B[36mgenerate_label_map\u001B[0m:\u001B[36m461\u001B[0m - \u001B[33m\u001B[1mIterating over all elements in the column to generate label map. This can be slow for large columns. Consider `dataset['my_col'].freeze_label_map({0: 'dog'})` or `dataset['my_col'].freeze_labels(['dog'])` to skip label map generation.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Added pseudo dataset: 150 images, total: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating label map from unique labels: 100%|██████████| 200/200 [00:00<00:00, 5431.24it/s]\n",
      "\u001B[32m2025-07-11 13:35:16.484\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl.datasets.columns.base_column\u001B[0m:\u001B[36m_generate_label_map_from_unique_labels\u001B[0m:\u001B[36m457\u001B[0m - \u001B[1mGenerated label map: {0: 'AboveGround', 1: 'Defect', 2: 'Overgrown', 3: 'Stone', 4: 'Tip'}\u001B[0m\n",
      "\u001B[32m2025-07-11 13:35:16.484\u001B[0m | \u001B[33m\u001B[1mWARNING \u001B[0m | \u001B[36monedl.datasets.columns.base_column\u001B[0m:\u001B[36mgenerate_label_map\u001B[0m:\u001B[36m461\u001B[0m - \u001B[33m\u001B[1mIterating over all elements in the column to generate label map. This can be slow for large columns. Consider `dataset['my_col'].freeze_label_map({0: 'dog'})` or `dataset['my_col'].freeze_labels(['dog'])` to skip label map generation.\u001B[0m\n",
      "Generating label map from unique labels: 100%|██████████| 200/200 [00:00<00:00, 1155455.65it/s]\n",
      "\u001B[32m2025-07-11 13:35:16.485\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl.datasets.columns.base_column\u001B[0m:\u001B[36m_generate_label_map_from_unique_labels\u001B[0m:\u001B[36m457\u001B[0m - \u001B[1mGenerated label map: {0: 'AboveGround'}\u001B[0m\n",
      "\u001B[32m2025-07-11 13:35:16.498\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36msave\u001B[0m:\u001B[36m191\u001B[0m - \u001B[1mSaved dataset train-f0:1 to local store.\u001B[0m\n",
      "\u001B[32m2025-07-11 13:35:16.498\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mresolve_latest_version\u001B[0m:\u001B[36m577\u001B[0m - \u001B[1mResolved latest version of dataset train-f0 to 1 local='daniel-osman---streamlining-annotation-bootstrapping/pipeline-test'\u001B[0m\n",
      "\u001B[32m2025-07-11 13:35:16.517\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m429\u001B[0m - \u001B[1mPushing 294 blobs to remote.\u001B[0m\n",
      "Files Uploaded:   0%|          | 0/295 [00:00<?, ?file/s]\n",
      "Files Confirmed:   0%|          | 0/295 [00:00<?, ?file/s]\u001B[A\n",
      "\n",
      "Getting upload links:   0%|          | 0/295 [00:00<?, ?file/s]\u001B[A\u001B[A\n",
      "\n",
      "Getting upload links:  87%|████████▋ | 256/295 [00:00<00:00, 680.14file/s]\u001B[A\u001B[A\n",
      "\n",
      "Files Uploaded:  68%|██████▊   | 200/295 [00:00<00:00, 329.03file/s]      \u001B[A\u001B[A\n",
      "Files Uploaded:  99%|█████████▊| 291/295 [00:02<00:00, 69.05file/s] ]\u001B[A\n",
      "\n",
      "Confirming files:   0%|          | 0/95 [00:00<?, ?file/s]\u001B[A\u001B[A\n",
      "\n",
      "Confirming files: 100%|██████████| 95/95 [00:00<00:00, 141.94file/s]\u001B[A\u001B[A\n",
      "\n",
      "                                                                    \u001B[A\u001B[A\n",
      "                                                                   ] \u001B[A\r\n",
      "                                                                    \u001B[A\u001B[32m2025-07-11 13:35:19.929\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m440\u001B[0m - \u001B[1mPushing dataset train-f0:1 to remote.\u001B[0m\n",
      "\u001B[32m2025-07-11 13:35:20.307\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m459\u001B[0m - \u001B[1mPushed dataset remote as train-f0:1 -> train-f0:1.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ TRAINING DATASET REBUILT:\n",
      "  - Initial GT: 50 images\n",
      "  - Pseudo labels: 150 images\n",
      "  - Total: 200 images\n",
      "  - Saved as: train-f0\n",
      "  - Label map: {0: 'AboveGround', 1: 'Defect', 2: 'Overgrown', 3: 'Stone', 4: 'Tip'}\n",
      "==================================================\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.5 Train Updated Model\n",
    "Train a new model on the expanded training set"
   ],
   "id": "b501662afac7aa99"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T12:19:55.908899Z",
     "start_time": "2025-07-11T11:35:39.065340Z"
    }
   },
   "cell_type": "code",
   "source": "pipeline.train_model()",
   "id": "7e822708055ad554",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-07-11 13:35:39.424\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mresolve_latest_version\u001B[0m:\u001B[36m592\u001B[0m - \u001B[1mResolved latest version of dataset train-f0 to 1 with remote='daniel-osman---streamlining-annotation-bootstrapping/pipeline-test'.\u001B[0m\n",
      "\u001B[32m2025-07-11 13:35:39.968\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl._local_store.datasets\u001B[0m:\u001B[36mresolve_latest_version\u001B[0m:\u001B[36m592\u001B[0m - \u001B[1mResolved latest version of dataset train-f0 to 1 with remote='daniel-osman---streamlining-annotation-bootstrapping/pipeline-test'.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training FasterRCNNConfig on dataset: train-f0\n",
      "Configuration: 30 epochs, batch size 6\n",
      "Backbone: RESNET_50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-07-11 13:35:42.478\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl.client.operations.clients._common\u001B[0m:\u001B[36mcreate_event_stream\u001B[0m:\u001B[36m79\u001B[0m - \u001B[1mSubscribing to job events...\u001B[0m\n",
      "\u001B[32m2025-07-11 13:35:42.478\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl.client.operations.clients._common\u001B[0m:\u001B[36mcreate_event_stream\u001B[0m:\u001B[36m80\u001B[0m - \u001B[1mJob chill-muffler-0 in WAITING state\u001B[0m\n",
      "\u001B[32m2025-07-11 13:35:43.632\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl.client.operations.clients._common\u001B[0m:\u001B[36mcreate_event_stream\u001B[0m:\u001B[36m84\u001B[0m - \u001B[1mJob chill-muffler-0 in RUNNING state\u001B[0m\n",
      "\u001B[32m2025-07-11 14:19:54.672\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl.client.operations.clients._common\u001B[0m:\u001B[36mcreate_event_stream\u001B[0m:\u001B[36m84\u001B[0m - \u001B[1mJob chill-muffler-0 in DONE state\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job submitted\n",
      "Model UID: chill-muffler-0\n",
      "Training job state: DONE\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.6 Evaluate Performance",
   "id": "7a4feb9510ede0a0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T13:14:48.566206Z",
     "start_time": "2025-07-11T13:13:50.748449Z"
    }
   },
   "cell_type": "code",
   "source": "pipeline.evaluate_model()",
   "id": "daa83206d911d917",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating chill-muffler-0 on val:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-07-11 15:13:53.167\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl.client.operations.clients._common\u001B[0m:\u001B[36mcreate_event_stream\u001B[0m:\u001B[36m79\u001B[0m - \u001B[1mSubscribing to job events...\u001B[0m\n",
      "\u001B[32m2025-07-11 15:13:53.168\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl.client.operations.clients._common\u001B[0m:\u001B[36mcreate_event_stream\u001B[0m:\u001B[36m80\u001B[0m - \u001B[1mJob furious-henry-0 in WAITING state\u001B[0m\n",
      "\u001B[32m2025-07-11 15:13:53.889\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl.client.operations.clients._common\u001B[0m:\u001B[36mcreate_event_stream\u001B[0m:\u001B[36m84\u001B[0m - \u001B[1mJob furious-henry-0 in RUNNING state\u001B[0m\n",
      "\u001B[32m2025-07-11 15:14:46.413\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36monedl.client.operations.clients._common\u001B[0m:\u001B[36mcreate_event_stream\u001B[0m:\u001B[36m84\u001B[0m - \u001B[1mJob furious-henry-0 in DONE state\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation job submitted\n",
      "Evaluation UID: furious-henry-0\n",
      "Evaluation job state: DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete\n",
      "Report URL: https://21e007818fa1dd0840eac0d6d59ba986.eu.r2.cloudflarestorage.com/onedl-data/daniel-osman---streamlining-annotation-bootstrapping/pipeline-test/-/19e6aefea00922c219416e00ce1ddf9c.html?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=bb17714b86b2e84a836c55404335cef8%2F20250711%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20250711T131448Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=b2b5ad59cc4303c3a2d2365c40cb698f79e866a142de7644da48f1dc320bfa49\n",
      "Metrics: {\"mAP50\": 0.1888017474404626, \"mAP75\": 0.09925452144850186, \"mAP_all\": 0.103202114225167, \"fn_count\": 35, \"fp_count\": 141, \"tp_count\": 103}\n",
      "✓ Iteration automatically marked as COMPLETED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.7 Status",
   "id": "1357a4f3c2cd78ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T13:19:02.183452Z",
     "start_time": "2025-07-11T13:19:02.180586Z"
    }
   },
   "cell_type": "code",
   "source": "pipeline.get_pipeline_status()",
   "id": "843955a711b93b5f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PIPELINE STATUS REPORT\n",
      "==================================================\n",
      "Flow ID: f0\n",
      "Current Iteration: 2\n",
      "Current Status: SAMPLING_COMPLETE\n",
      "Training Dataset: train-f0\n",
      "Current Model UID: chill-muffler-0\n",
      "Training Configuration: {'model_type': 'FasterRCNNConfig', 'task_type': 'object_detection', 'backbone': 'RESNET_50', 'epochs': 6, 'batch_size': 6}\n",
      "Database Path: pseudo_labeling_metadata_ptest.db\n",
      "Ground Truth Images: 50\n",
      "Pseudo-labeled Images: 300\n",
      "Total Training Images: 350\n",
      "Sample Size Per Iteration: 150\n",
      "Minimum Confidence Threshold: 0.5\n",
      "\n",
      "RECENT ITERATIONS:\n",
      "  Iteration 2: SAMPLING_COMPLETE\n",
      "  Iteration 1: COMPLETED (completed: 2025-07-11 15:14:48)\n",
      "  Iteration 0: COMPLETED (completed: 2025-07-11 13:21:04)\n",
      "==================================================\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Additional Runs\n",
    "To run additional iterations, repeat Section 2 after logging. For creating a new flow, go back to Section 1, update the current_flow and go again.\n"
   ],
   "id": "bcb41c36fe5ccd64"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
